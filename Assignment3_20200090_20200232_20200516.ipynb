{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#ID : 20200090 + 20200232 + 20200516\n",
        "#\n",
        "# S: 5\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "\n",
        "#load\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "#Standardize\n",
        "std=np.std(X_train, axis=0)\n",
        "std=np.where(std==0,1,std)\n",
        "X_train=(X_train - np.mean(X_train,axis=0))/std\n",
        "\n",
        "std1=np.std(X_test, axis=0)\n",
        "std1=np.where(std1==0,1,std1)\n",
        "X_test=(X_test - np.mean(X_test,axis=0))/std1\n",
        "# reshape\n",
        "X_train = X_train.reshape(-1, 28*28)\n",
        "X_test = X_test.reshape(-1, 28*28)\n",
        "\n",
        "# Divide data \n",
        "def train_val_split(X_train,y_train,split_ratio =0.8):\n",
        "    np.random.seed(1)\n",
        "    shuffle_indices = np.random.permutation(len(y_train))\n",
        "    X_shuffled = X_train[shuffle_indices]\n",
        "    y_shuffled = y_train[shuffle_indices]\n",
        "    split_index = int(len(y_train) * split_ratio)\n",
        "    # Split the data\n",
        "    X_train = X_shuffled[:split_index]\n",
        "    y_train = y_shuffled[:split_index]\n",
        "    X_val = X_shuffled[split_index:]\n",
        "    y_val = y_shuffled[split_index:]\n",
        "\n",
        "    return X_train, X_val, y_train, y_val\n",
        "X_train, X_val, y_train, y_val =train_val_split(X_train,y_train, 0.8)\n",
        "\n",
        "num_classes = 10\n",
        "\"\"\"\n",
        "#one hot vector\n",
        "y_train_encoded = np.eye(num_classes)[y_train.astype(int)]\n",
        "y_val_encoded = np.eye(num_classes)[y_val.astype(int)]\n",
        "y_test_encoded = np.eye(num_classes)[y_test.astype(int)]\n",
        "\"\"\"\n",
        "\n",
        "# implement one hot vector\n",
        "def one_hot_encode(labels, num_classes):\n",
        "    num_samples = len(labels)\n",
        "    encoded_labels = np.zeros((num_samples, num_classes))\n",
        "    encoded_labels[np.arange(num_samples), labels] = 1\n",
        "    return encoded_labels\n",
        "y_train_encoded = one_hot_encode(y_train, num_classes)\n",
        "y_val_encoded = one_hot_encode(y_val, num_classes)\n",
        "y_test_encoded = one_hot_encode(y_test, num_classes)\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "def initialize_weights(layer_sizes):\n",
        "    np.random.seed(1)\n",
        "    w = []\n",
        "    b = []\n",
        "    for i in range(1, len(layer_sizes)):\n",
        "        input_size = layer_sizes[i - 1]\n",
        "        output_size = layer_sizes[i]\n",
        "        w_matrix = np.random.randn(input_size, output_size)\n",
        "        b_matrix = np.random.randn(output_size)\n",
        "        w.append(w_matrix)\n",
        "        b.append(b_matrix)\n",
        "    return w, b\n",
        "\n",
        "def softmax(x):\n",
        "    exp_scores = np.exp(x)\n",
        "    probabilities = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "    return probabilities\n",
        "\n",
        "def forward(X, w,b):\n",
        "    a = [X] #a[0] for layer zero\n",
        "    for i in range(len(w)):#num_layers\n",
        "        phiz = sigmoid(np.dot(a[i], w[i]) + b[i])\n",
        "        a.append(phiz)\n",
        "    return a\n",
        "\n",
        "def backward(X, y_true, a, w, b):\n",
        "    num_layers = len(w)\n",
        "    num_samples = X.shape[0]\n",
        "    dw = []\n",
        "    db = []\n",
        "    error = y_true - a[-1] #at the last layer = dL_da\n",
        "    dz = error * sigmoid_derivative(a[-1]) # dL_dz = (dL_da * da_dz)\n",
        "    dw.insert(0, np.dot(a[-2].T, dz)) #d_w=(dL_dz * x)   respect to   # x=dz_dw\n",
        "    db.insert(0, np.sum(dz, axis=0))\n",
        "    # use chain rule to back\n",
        "    for i in range(num_layers - 2, -1, -1):\n",
        "        error = np.dot(dz, w[i + 1].T) # dz_da\n",
        "        dz = error * sigmoid_derivative(a[i + 1])\n",
        "        dw.insert(0, np.dot(a[i].T, dz))\n",
        "        db.insert(0, np.sum(dz, axis=0))\n",
        "\n",
        "    return dw, db\n",
        "\n",
        "\n",
        "\n",
        "def update_weights(w, b, dw, db, lr):\n",
        "    num_layers = len(w)\n",
        "    for i in range(num_layers):\n",
        "        w[i] += lr * dw[i]\n",
        "        b[i] += lr * db[i]\n",
        "    return w, b\n",
        "\n",
        "def train(X, y, num_of_layers, size_of_layers, lr, num_epochs, batch_size, Xy_val=None):\n",
        "    w, b = initialize_weights(size_of_layers)\n",
        "    num_batches = X.shape[0] // batch_size\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch in range(num_batches):\n",
        "            start_idx = batch * batch_size\n",
        "            end_idx = start_idx + batch_size\n",
        "            X_batch = X[start_idx:end_idx]\n",
        "            y_batch = y[start_idx:end_idx]\n",
        "\n",
        "            # Forward \n",
        "            a = forward(X_batch, w, b)\n",
        "\n",
        "            # Backward\n",
        "            dw, db = backward(X_batch, y_batch, a, w, b)\n",
        "\n",
        "            # Update w and b\n",
        "            w, b = update_weights(w, b, dw, db, lr)\n",
        "\n",
        "        # Compute training loss\n",
        "        train_yp = forward(X, w, b) #a=yp\n",
        "        train_loss = mse_loss(y, train_yp[-1])\n",
        "\n",
        "        if Xy_val:\n",
        "            # Compute validation loss\n",
        "            val_yp = forward(Xy_val[0], w, b)\n",
        "            val_loss = mse_loss(Xy_val[1], val_yp[-1])\n",
        "            print(f\"Epoch {epoch + 1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
        "        else:\n",
        "            print(f\"Epoch {epoch + 1}/{num_epochs} - Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "    return w, b\n",
        "\n",
        "def NN(x, y, num_of_layers, size_of_layers):\n",
        "    num_features = x.shape[1]\n",
        "    layer_sizes = [num_features] + size_of_layers #[784, size_of_layers]\n",
        "    print(layer_sizes)\n",
        "    w_final,b_final = train(x, y, num_of_layers, layer_sizes, lr=0.1, num_epochs=10, batch_size=32, Xy_val=(X_val, y_val_encoded))\n",
        "\n",
        "    # Testing\n",
        "    test_yp = forward(X_test, w_final,b_final)\n",
        "    test_loss = mse_loss(y_test_encoded, test_yp[-1])\n",
        "    #accuracy = np.mean(y_test_encoded== test_yp[-1])\n",
        "    accuracy = 1 - test_loss\n",
        "    \n",
        "    #test_yp[-1] =softmax(test_yp[-1])\n",
        "    test_yp[-1]=np.argmax(test_yp[-1], axis=1)\n",
        "    print(\"y_true\")\n",
        "    print(np.argmax(y_test_encoded, axis=1))\n",
        "    print(\"Y_pred\")\n",
        "    print(test_yp[-1])\n",
        "    test_yp[-1]=np.eye(10)[test_yp[-1]]\n",
        "    print(\"Y_pred_encoded\")\n",
        "    print(test_yp[-1])\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.2%}\")\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "architectures = [\n",
        "    (2, [20,10 ], \"Architecture 1\"),\n",
        "    (3, [20, 15,10], \"Architecture 2\"),\n",
        "    (3, [15, 20,10], \"Architecture 3\")\n",
        "]\n",
        "\n",
        "for num_layers, layer_sizes, architecture_name in architectures:\n",
        "    print(f\"\\n{architecture_name}\")\n",
        "    accuracy = NN(X_train, y_train_encoded, num_layers, layer_sizes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F47Y4HDo9LW",
        "outputId": "4a8c06f6-f44b-4f68-8a18-55405cd0577f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "\n",
            "Architecture 1\n",
            "[784, 20, 10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-ab697be6d497>:56: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-z))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Train Loss: 0.0280 - Val Loss: 0.0289\n",
            "Epoch 2/10 - Train Loss: 0.0204 - Val Loss: 0.0221\n",
            "Epoch 3/10 - Train Loss: 0.0177 - Val Loss: 0.0197\n",
            "Epoch 4/10 - Train Loss: 0.0162 - Val Loss: 0.0184\n",
            "Epoch 5/10 - Train Loss: 0.0151 - Val Loss: 0.0175\n",
            "Epoch 6/10 - Train Loss: 0.0143 - Val Loss: 0.0170\n",
            "Epoch 7/10 - Train Loss: 0.0138 - Val Loss: 0.0167\n",
            "Epoch 8/10 - Train Loss: 0.0132 - Val Loss: 0.0161\n",
            "Epoch 9/10 - Train Loss: 0.0128 - Val Loss: 0.0160\n",
            "Epoch 10/10 - Train Loss: 0.0124 - Val Loss: 0.0158\n",
            "y_true\n",
            "[7 2 1 ... 4 5 6]\n",
            "Y_pred\n",
            "[7 2 1 ... 4 5 6]\n",
            "Y_pred_encoded\n",
            "[[0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Test Loss: 0.0149\n",
            "Accuracy: 98.51%\n",
            "\n",
            "Architecture 2\n",
            "[784, 20, 15, 10]\n",
            "Epoch 1/10 - Train Loss: 0.0261 - Val Loss: 0.0273\n",
            "Epoch 2/10 - Train Loss: 0.0192 - Val Loss: 0.0206\n",
            "Epoch 3/10 - Train Loss: 0.0168 - Val Loss: 0.0185\n",
            "Epoch 4/10 - Train Loss: 0.0152 - Val Loss: 0.0172\n",
            "Epoch 5/10 - Train Loss: 0.0143 - Val Loss: 0.0167\n",
            "Epoch 6/10 - Train Loss: 0.0135 - Val Loss: 0.0162\n",
            "Epoch 7/10 - Train Loss: 0.0128 - Val Loss: 0.0160\n",
            "Epoch 8/10 - Train Loss: 0.0123 - Val Loss: 0.0155\n",
            "Epoch 9/10 - Train Loss: 0.0117 - Val Loss: 0.0153\n",
            "Epoch 10/10 - Train Loss: 0.0115 - Val Loss: 0.0151\n",
            "y_true\n",
            "[7 2 1 ... 4 5 6]\n",
            "Y_pred\n",
            "[7 2 1 ... 4 5 6]\n",
            "Y_pred_encoded\n",
            "[[0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Test Loss: 0.0146\n",
            "Accuracy: 98.54%\n",
            "\n",
            "Architecture 3\n",
            "[784, 15, 20, 10]\n",
            "Epoch 1/10 - Train Loss: 0.0280 - Val Loss: 0.0284\n",
            "Epoch 2/10 - Train Loss: 0.0204 - Val Loss: 0.0216\n",
            "Epoch 3/10 - Train Loss: 0.0177 - Val Loss: 0.0190\n",
            "Epoch 4/10 - Train Loss: 0.0160 - Val Loss: 0.0178\n",
            "Epoch 5/10 - Train Loss: 0.0147 - Val Loss: 0.0167\n",
            "Epoch 6/10 - Train Loss: 0.0143 - Val Loss: 0.0164\n",
            "Epoch 7/10 - Train Loss: 0.0136 - Val Loss: 0.0161\n",
            "Epoch 8/10 - Train Loss: 0.0128 - Val Loss: 0.0158\n",
            "Epoch 9/10 - Train Loss: 0.0124 - Val Loss: 0.0154\n",
            "Epoch 10/10 - Train Loss: 0.0122 - Val Loss: 0.0154\n",
            "y_true\n",
            "[7 2 1 ... 4 5 6]\n",
            "Y_pred\n",
            "[7 2 1 ... 4 5 6]\n",
            "Y_pred_encoded\n",
            "[[0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Test Loss: 0.0152\n",
            "Accuracy: 98.48%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Architecture 2 have higher Accuracy than Architecture 3 :\n",
        "Because of this reason\n",
        "\n",
        "#1-Dimensionality reduction: \n",
        "By reducing the number of nodes in a layer, you effectively reduce the dimensionality of the representation in that layer. This can help in compressing the information and extracting the most relevant features, allowing for a more compact and efficient representation of the data.\n",
        "\n",
        "#2-Computational efficiency: \n",
        "A smaller layer requires fewer computations compared to a larger layer. This can lead to faster training and inference times, especially when dealing with large-scale neural networks or limited computational resources.\n",
        "\n",
        "#3-Improved generalization: \n",
        "With a smaller layer, the model is encouraged to learn more abstract and generalizable representations of the data. This can enhance the model's ability to generalize well to unseen data and perform better on test or validation datasets."
      ],
      "metadata": {
        "id": "O7EEMMA7AZYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cross entropy in backward\n",
        "\n",
        "def backward(X, y_true, a, w, b):\n",
        "    num_layers = len(w)\n",
        "    num_samples = X.shape[0]\n",
        "    dw = []\n",
        "    db = []\n",
        "    error =(- y_true / a[-1])+((1 - y_true) / (1 - a[-1])) #at the last layer = dL_da\n",
        "    dz = error * sigmoid_derivative(a[-1]) # dL_dz = (dL_da * da_dz)\n",
        "    dw.insert(0, np.dot(a[-2].T, dz)) #d_w=(dL_dz * x)   respect to   # x=dz_dw\n",
        "    db.insert(0, np.sum(dz, axis=0))\n",
        "    # use chain rule to back\n",
        "    for i in range(num_layers - 2, -1, -1):\n",
        "        error = np.dot(dz, w[i + 1].T) # dz_da\n",
        "        dz = error * sigmoid_derivative(a[i + 1])\n",
        "        dw.insert(0, np.dot(a[i].T, dz))\n",
        "        db.insert(0, np.sum(dz, axis=0))\n",
        "\n",
        "    return dw, db\n",
        "\n"
      ],
      "metadata": {
        "id": "ohmrrIebFjR1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}